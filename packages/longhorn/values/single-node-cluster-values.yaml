global:
  cattle:
    systemDefaultRegistry: ""
    windowsCluster:
      # Enable this to allow Longhorn to run on the Rancher deployed Windows cluster
      enabled: false
      # Tolerate Linux node taint
      tolerations:
      - key: "cattle.io/os"
        value: "linux"
        effect: "NoSchedule"
        operator: "Equal"
      # Select Linux nodes
      nodeSelector:
        kubernetes.io/os: "linux"
      # Recognize toleration and node selector for Longhorn run-time created components
      defaultSetting:
        taintToleration: cattle.io/os=linux:NoSchedule
        systemManagedComponentsNodeSelector: kubernetes.io/os:linux

image:
  longhorn:
    engine:
      repository: ###ZARF_REGISTRY###/longhornio/longhorn-engine
      tag: v1.5.1
    manager:
      repository: ###ZARF_REGISTRY###/longhornio/longhorn-manager
      tag: v1.5.1
    ui:
      repository: ###ZARF_REGISTRY###/longhornio/longhorn-ui
      tag: v1.5.1
    instanceManager:
      repository: ###ZARF_REGISTRY###/longhornio/longhorn-instance-manager
      tag: v1.5.1
    shareManager:
      repository: ###ZARF_REGISTRY###/longhornio/longhorn-share-manager
      tag: v1.5.1
    backingImageManager:
      repository: ###ZARF_REGISTRY###/longhornio/backing-image-manager
      tag: v1.5.1
    supportBundleKit:
      repository: ###ZARF_REGISTRY###/longhornio/support-bundle-kit
      tag: v0.0.25
  csi:
    attacher:
      repository: ###ZARF_REGISTRY###/longhornio/csi-attacher
      tag: v4.2.0
    provisioner:
      repository: ###ZARF_REGISTRY###/longhornio/csi-provisioner
      tag: v3.4.1
    nodeDriverRegistrar:
      repository: ###ZARF_REGISTRY###/longhornio/csi-node-driver-registrar
      tag: v2.7.0
    resizer:
      repository: ###ZARF_REGISTRY###/longhornio/csi-resizer
      tag: v1.7.0
    snapshotter:
      repository: ###ZARF_REGISTRY###/longhornio/csi-snapshotter
      tag: v6.2.1
    livenessProbe:
      repository: ###ZARF_REGISTRY###/longhornio/livenessprobe
      tag: v2.9.0
  pullPolicy: IfNotPresent

service:
  ui:
    type: ClusterIP
    nodePort: null
  manager:
    type: ClusterIP
    nodePort: ""
    loadBalancerIP: ""
    loadBalancerSourceRanges: ""

persistence:
  defaultClass: true
  defaultFsType: ext4
  defaultMkfsParams: ""
  defaultClassReplicaCount: 3
  defaultDataLocality: disabled # best-effort otherwise
  reclaimPolicy: Delete
  migratable: false
  recurringJobSelector:
    enable: false
    jobList: []
  backingImage:
    enable: false
    name: ~
    dataSourceType: ~
    dataSourceParameters: ~
    expectedChecksum: ~
  defaultNodeSelector:
    enable: false # disable by default
    selector: []
  removeSnapshotsDuringFilesystemTrim: ignored # "enabled" or "disabled" otherwise

csi:
  kubeletRootDir: "/var/lib/kubelet"
  attacherReplicaCount: ~
  provisionerReplicaCount: ~
  resizerReplicaCount: ~
  snapshotterReplicaCount: ~

defaultSettings:
  backupTarget: ~
  backupTargetCredentialSecret: ~
  allowRecurringJobWhileVolumeDetached: ~
  createDefaultDiskLabeledNodes: ~
  defaultDataPath: ~
  defaultDataLocality: ~
  replicaSoftAntiAffinity: ~
  replicaAutoBalance: ~
  storageOverProvisioningPercentage: ~
  storageMinimalAvailablePercentage: ~
  upgradeChecker: ~
  ### Set defaultReplicaCount because there is a single node
  defaultReplicaCount: 1
  defaultLonghornStaticStorageClass: ~
  backupstorePollInterval: ~
  failedBackupTTL: ~
  restoreVolumeRecurringJobs: ~
  recurringSuccessfulJobsHistoryLimit: ~
  recurringFailedJobsHistoryLimit: ~
  supportBundleFailedHistoryLimit: ~
  taintToleration: ~
  systemManagedComponentsNodeSelector: ~
  ### Set priorityClass to longhorn-high in order to ensure Longhorn 
  ### is scheduled without any delays.
  priorityClass: longhorn-high
  autoSalvage: ~
  autoDeletePodWhenVolumeDetachedUnexpectedly: ~
  disableSchedulingOnCordonedNode: ~
  replicaZoneSoftAntiAffinity: ~
  nodeDownPodDeletionPolicy: ~
  allowNodeDrainWithLastHealthyReplica: ~
  mkfsExt4Parameters: ~
  disableReplicaRebuild: ~
  replicaReplenishmentWaitInterval: ~
  concurrentReplicaRebuildPerNodeLimit: ~
  concurrentVolumeBackupRestorePerNodeLimit: ~
  disableRevisionCounter: ~
  systemManagedPodsImagePullPolicy: ~
  allowVolumeCreationWithDegradedAvailability: ~
  autoCleanupSystemGeneratedSnapshot: ~
  concurrentAutomaticEngineUpgradePerNodeLimit: ~
  backingImageCleanupWaitInterval: ~
  backingImageRecoveryWaitInterval: ~
  guaranteedEngineManagerCPU: ~
  guaranteedReplicaManagerCPU: ~
  kubernetesClusterAutoscalerEnabled: ~
  orphanAutoDeletion: ~
  storageNetwork: ~
  deletingConfirmationFlag: ~
  engineReplicaTimeout: ~
  snapshotDataIntegrity: ~
  snapshotDataIntegrityImmediateCheckAfterSnapshotCreation: ~
  snapshotDataIntegrityCronjob: ~
  removeSnapshotsDuringFilesystemTrim: ~
  fastReplicaRebuildEnabled: ~
  replicaFileSyncHttpClientTimeout: ~
privateRegistry:
  createSecret: ~
  registryUrl: ~
  registryUser: ~
  registryPasswd: ~
  registrySecret: private-registry


# # Pod scheduling and node constraints tailored for a single node
# kubernetes:
#   storageClass: 
#     isDefaultClass: true  # This can be set to true if you want Longhorn to be the default storage class
#   # Ensuring that all Longhorn components are scheduled on the single available node
#   nodeSelector:
#     kubernetes.io/hostname: "your-node-name"  # Replace with your actual node name
#   tolerations:
#     - key: "node-role.kubernetes.io/control-plane"
#       operator: "Exists"
#       effect: "NoSchedule"
#     - key: "node-role.kubernetes.io/master"
#       operator: "Exists"
#       effect: "NoSchedule"
#   nodeTag: "your-node-tag"
#   diskTag: "your-disk-tag"

# Adjusted resources requests and limits for single-node setup
resources:
  manager:
    requests:
      cpu: "100m"
      memory: "200Mi"
    limits:
      cpu: "200m"
      memory: "500Mi"
  driver:
    requests:
      cpu: "100m"
      memory: "100Mi"
    limits:
      cpu: "200m"
      memory: "200Mi"
  ui:
    requests:
      cpu: "50m"
      memory: "100Mi"
    limits:
      cpu: "100m"
      memory: "200Mi"


# # Network settings for a single-node setup
# network:
#   policy:
#     allowFromOtherNamespaces: true
