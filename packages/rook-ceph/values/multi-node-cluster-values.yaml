# See the UDS RKE2 repository's documentation, `docs/ROOK-CEPH.md` for more details on values overrides

imagePullSecrets:
  - name: private-registry

toolbox:
  enabled: true
  # TODO: setup renovate
  image: ###ZARF_REGISTRY###/ironbank/opensource/ceph/ceph:v18.2.2

cephClusterSpec:
  mon:
    count: ###ZARF_VAR_REPLICAS###
    allowMultiplePerNode: true

  mgr:
    count: ###ZARF_VAR_REPLICAS###
    allowMultiplePerNode: true
    modules:
      - name: rook
        enabled: true

  cephVersion:
    # TODO: setup renovate
    image: ###ZARF_REGISTRY###/ironbank/opensource/ceph/ceph:v18.2.2
    allowUnsupported: true

  dashboard:
    enabled: true
    ssl: false

  monitoring:
    enabled: ###ZARF_VAR_ENABLE_MONITORING###
    rulesNamespace: rook-ceph
    createPrometheusRules: false

  network:
    # Use host networking to avoid CNI causing storage issues
    # Equivalent to legacy `hostNetwork: true`
    provider: host
    connections:
      encryption:
        enabled: true

  storage:
    useAllNodes: true
    useAllDevices: true
    devices:
      - name: ###ZARF_VAR_DEVICE_NAME###
    config:
      osdsPerDevice: "1"

cephBlockPool:
  enabled: true
  spec:
    failureDomain: host
    isDefault: true
    replicated:
      size: ###ZARF_VAR_REPLICAS###

cephFileSystem:
  enabled: true
  spec:
    dataPools:
      - failureDomain: host
        replicated:
          size: ###ZARF_VAR_REPLICAS###
        compressionMode: none
    metadataPool:
      replicated:
        size: ###ZARF_VAR_REPLICAS###
    metadataServer:
      activeCount: ###ZARF_VAR_ACTIVE_METADATA_SERVERS###
      activeStandby: false
  storageClass:
    isDefault: false

cephObjectStore:
  enabled: true
  spec:
    dataPools:
      replicated:
        size: ###ZARF_VAR_REPLICAS###
    metadataPool:
      replicated:
        size: ###ZARF_VAR_REPLICAS###
      failureDomain: host
    dataPool:
      failureDomain: host
    gateway:
      instances: 1
  storageClass:
    isDefault: false
    parameters:
      region: ###ZARF_VAR_REGION###
